{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BobGanti/ColabNotebooks/blob/main/CRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqyWMYpXfP0Q"
      },
      "source": [
        "# Implementation For CRAG <br>(Corrective Retrieval Augmentation Generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWvCtQu9c3MI"
      },
      "source": [
        "### Mounting the drive and setting up the environment variables (Colab specific)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsxNHYYxFOoU"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "langchain langchain-community openai \\\n",
        "transformers sentence-transformers \\\n",
        "rank-bm25 \\\n",
        "accelerate -U \\\n",
        "PyMuPDF PyPDF2 \\\n",
        "beautifulsoup4 \\\n",
        "datasets \\\n",
        "torch==2.4.1 torchaudio torchvision pyarrow \\\n",
        "google-api-python-client \\\n",
        "requests \\\n",
        "faiss-cpu faiss-gpu \\\n",
        "scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE1fIXW-d5Cx"
      },
      "source": [
        "### Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVcr3fX-FZEm"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive, userdata\n",
        "drive = drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DATA_DIR = userdata.get('ROOT_DIR')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GOOGLE_SEARCH_API_KEY = userdata.get('GOOGLE_SEARCH_API_KEY')\n",
        "GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
        "arxiv_DIR = DATA_DIR + \"/arxiv\"\n",
        "PDF_DIR = arxiv_DIR + \"/PDFs\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc1Kg4iaq3Ko"
      },
      "source": [
        "### Class Definitions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import pickle\n",
        "import pymupdf\n",
        "import uuid\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from torch.utils.data import Dataset\n",
        "import transformers\n",
        "import faiss\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "15ycqqFRMjay"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Lub9Prs0B49L"
      },
      "outputs": [],
      "source": [
        "global_chunk_counter = 0\n",
        "\n",
        "# Chunk class\n",
        "class Chunk:\n",
        "    def __init__(self, text, metadata, embedding):\n",
        "        self.text = text\n",
        "        self.embedding = embedding\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Chunk({self.metadata['chunk_id']}, {self.metadata['title']}, Page {self.metadata['page_number']})\"\n",
        "\n",
        "class Document:\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.title = os.path.splitext(os.path.basename(path))[0]\n",
        "        self.document_id = f\"{self.title.replace(' ', '_')}#{str(uuid.uuid4())[:10]}\"\n",
        "        self.chunks = []\n",
        "        self._extract_and_chunk_text(chunk_size=2500, chunk_overlap=100)\n",
        "\n",
        "\n",
        "    def _extract_and_chunk_text(self, chunk_size=2500, chunk_overlap=100):\n",
        "        global global_chunk_counter\n",
        "        document = pymupdf.open(self.path)\n",
        "        separators = ['\\n\\n', '\\n', ' ', '']\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=separators\n",
        "        )\n",
        "        print(f\"\\n***** Processing: {self.title} *****\\n\")\n",
        "        num_chunks = 0\n",
        "        for num in range(len(document)):\n",
        "            page = document.load_page(num)\n",
        "            page_text = self.get_page_text(page)\n",
        "            page_chunks = text_splitter.split_text(page_text)\n",
        "            for i, chunk in enumerate(page_chunks):\n",
        "                page_number = num + 1\n",
        "                global_chunk_counter += 1\n",
        "\n",
        "                metadata = {\n",
        "                    \"title\": self.title,\n",
        "                    \"document_id\": self.document_id,\n",
        "                    \"page_number\": page_number,\n",
        "                    \"chunk_id\": f\"Pge-{page_number}-Chnk{num_chunks + 1}\",\n",
        "                    \"global_chunk_number\": global_chunk_counter\n",
        "                }\n",
        "                embed = EmbeddingModels(MODEL_NAME, chunk, flag='gpt').embeddings\n",
        "                newchunk = Chunk(chunk, metadata, embed)\n",
        "                self.chunks.append(newchunk)\n",
        "                print(f\"Chunk {global_chunk_counter} created: {metadata}\")\n",
        "                num_chunks += 1\n",
        "        print(f\"Total chunks created: {num_chunks}\")\n",
        "\n",
        "\n",
        "\n",
        "    def remove_section(self, text):\n",
        "        # Heuristic to remove the References section in the pdf file\n",
        "        text = re.split(r'References|Bibliography', text, flags=re.IGNORECASE)[0]\n",
        "        return text\n",
        "\n",
        "    def get_page_text(self, page):\n",
        "        text = page.get_text()\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "        text = self.remove_section(text)\n",
        "        return text\n",
        "\n",
        "    def get_chunks(self):\n",
        "        return self.chunks\n",
        "\n",
        "    def save_document(self, dir):\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "        file_path = os.path.join(dir, f'{self.document_id}.pkl')\n",
        "        with open(file_path, 'wb') as file:\n",
        "            pickle.dump(self, file)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document({self.title}, {len(self.chunks)} chunks)\"\n",
        "\n",
        "# EmbeddingModels class\n",
        "class EmbeddingModels:\n",
        "    def __init__(self, model, text, flag='gpt'):\n",
        "        self.model = model\n",
        "        self.flag = flag\n",
        "        if self.flag == 'gpt':\n",
        "            self.embeddings = self.__create_gpt_embedding(text)\n",
        "        elif self.flag == 'sentence':\n",
        "            self.embeddings = self.__create_sentence_embedding(text)\n",
        "\n",
        "    def __create_gpt_embedding(self, text):\n",
        "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "        embedding = client.embeddings.create(input=[text], model=self.model).data[0].embedding\n",
        "        return embedding\n",
        "\n",
        "    def __create_sentence_embedding(self, text):\n",
        "        embedding = self.model.encode(text, convert_to_tensor=True)\n",
        "        embedding_np = np.array(embedding).astype('float32')\n",
        "        return embedding_np\n",
        "\n",
        "# VectorStore class\n",
        "class VectorStore:\n",
        "    def __init__(self, dimension, index_file=arxiv_DIR + '/Vectors/faiss_index.idx'):\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index_file = index_file\n",
        "        self.metadata_store = []\n",
        "\n",
        "    def add_embeddings(self, embeddings, metadata):\n",
        "        embeddings = np.array(embeddings).astype('float32')\n",
        "        if len(embeddings.shape) == 1:\n",
        "            embeddings = np.expand_dims(embeddings, axis=0)\n",
        "            metadata = [metadata]\n",
        "        self.index.add(embeddings)\n",
        "        self.metadata_store.extend(metadata)\n",
        "\n",
        "    def query(self, embedding, top_k=5):\n",
        "        embedding_np = np.array([embedding]).astype('float32')\n",
        "        vectors = self.load_index()\n",
        "        distances, indices = vectors.search(embedding_np, top_k)\n",
        "        return distances, indices\n",
        "\n",
        "    def save_index(self):\n",
        "        try:\n",
        "            faiss.write_index(self.index, self.index_file)\n",
        "            return \"Index saved to disk.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error saving index: {e}\"\n",
        "\n",
        "\n",
        "    def load_index(self):\n",
        "        self.index = faiss.read_index(self.index_file)\n",
        "        return self.index\n",
        "\n",
        "# Custom dataset class\n",
        "class GPT2Dataset(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
        "        return item\n",
        "\n",
        "class EvaluatorEncoder:\n",
        "    # Function to encode the data using the fine-tuned tokenizer\n",
        "    def encode_data(self, tokenizer, data, max_length=512, is_test=False):\n",
        "        inputs = tokenizer(\n",
        "            [f\"Question: {q} Context: {c}\" for q, c in zip(data['Query'].tolist(), data['Chunk'].tolist())],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        if not is_test:  # Only convert labels if not test data\n",
        "            labels = torch.tensor(data['Relevance'].tolist())\n",
        "            inputs['labels'] = labels\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubs3vZPwSPhD"
      },
      "source": [
        "### PDFs Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "txDmsCLlaB2R"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to read PDFs from a directory and create Document objects\n",
        "def create_documents_from_pdf(pdf_dir, save_dir):\n",
        "    document_paths = [os.path.join(pdf_dir, file) for file in os.listdir(pdf_dir) if file.endswith('.pdf')]\n",
        "    print(\"Number of PDFs: \", len(document_paths))\n",
        "\n",
        "    documents = []\n",
        "    for path in document_paths:\n",
        "        document = Document(path)\n",
        "        document.save_document(save_dir)\n",
        "        documents.append(document)\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwlpGES4XFi3"
      },
      "source": [
        "### Create Documents and Saving them locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EQuJRFNUKEI-"
      },
      "outputs": [],
      "source": [
        "\n",
        "SAVE_DIR = arxiv_DIR + \"/Documents\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Creating documents and save them locally\n",
        "# reated_documents = documents = create_documents_from_pdf(PDF_DIR, SAVE_DIR)\n",
        "# print(\"Number of Documents Created: \", len(created_documents))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd06uuTDQ9nM"
      },
      "source": [
        "### Loading Saved Documents from Local Dir and explore its features\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load saved Document objects\n",
        "def load_saved_documents(dir):\n",
        "    documents = []\n",
        "    for filename in os.listdir(dir):\n",
        "        if filename.endswith('.pkl'):\n",
        "            file_path = os.path.join(dir, filename)\n",
        "            with open(file_path, 'rb') as file:\n",
        "                document = pickle.load(file)\n",
        "                documents.append(document)\n",
        "    return documents\n"
      ],
      "metadata": {
        "id": "4OqWoKpgFfu_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqxynHGNBuLK"
      },
      "outputs": [],
      "source": [
        "# Directory paths\n",
        "LOAD_DIR = arxiv_DIR + \"/Documents\"\n",
        "MODEL_NAME = \"text-embedding-3-small\"\n",
        "FLAG = 'gpt'\n",
        "\n",
        "# Load documents from the local directory\n",
        "loaded_documents = load_saved_documents(LOAD_DIR)\n",
        "print(\"Number of Documents: \", len(loaded_documents))\n",
        "\n",
        "# Verify embeddings\n",
        "\n",
        "print(loaded_documents[-1].chunks[-1].metadata)\n",
        "print(f\"Embeddings: {loaded_documents[-1].get_chunks()[-1].embedding[:5]}\")\n",
        "print(\"Text: \", loaded_documents[-1].chunks[0].text[:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcJxwWE9qTbB"
      },
      "source": [
        "### Vectorstore Credentials Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectorstore_credentials(FLAG='gpt'):\n",
        "\n",
        "    EmbeddingCredentials = {\n",
        "            'gpt':{\n",
        "                \"MODEL_NAME\":\"text-embedding-3-small\",\n",
        "                \"FAISS_INDEX_FILE\":arxiv_DIR + '/Vectors/faiss_index.idx'\n",
        "            },\n",
        "            \"sentence\":{\n",
        "                \"MODEL_NAME\":'all-MiniLM-L6-v2',\n",
        "                \"FAISS_INDEX_FILE\":arxiv_DIR + '/Vectors/faiss_index_sentence.idx'\n",
        "            }\n",
        "    }\n",
        "    return EmbeddingCredentials\n",
        "\n",
        "FLAG='gpt'\n",
        "EmbeddingCredentials = get_vectorstore_credentials(FLAG)\n",
        "MODEL_NAME = EmbeddingCredentials[FLAG]['MODEL_NAME']\n",
        "FAISS_INDEX_FILE = EmbeddingCredentials[FLAG]['FAISS_INDEX_FILE']\n",
        "os.makedirs(os.path.dirname(FAISS_INDEX_FILE), exist_ok=True)\n",
        "print(MODEL_NAME)\n",
        "print(FAISS_INDEX_FILE)\n"
      ],
      "metadata": {
        "id": "DFHhmPHmHCni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialising the Vectorstore and Adding the Documents"
      ],
      "metadata": {
        "id": "LJmX7QNsDqD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = len(EmbeddingModels(MODEL_NAME, \"hello\", flag='gpt').embeddings)\n",
        "\n",
        "vector_store = VectorStore(dimension=DIM, index_file=FAISS_INDEX_FILE)\n"
      ],
      "metadata": {
        "id": "vMvjofeU_S0G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def add_documents_to_vectorstore():\n",
        "    for doc in loaded_documents:\n",
        "        for chunk in doc.get_chunks():\n",
        "            vector_store.add_embeddings(chunk.embedding, chunk.metadata)\n",
        "\n",
        "    vector_store.save_index()\n",
        "    return vector_store\n",
        "print(\"Num vectores before adding embeddings: \", vector_store.index.ntotal)\n",
        "# vector_store = add_documents_to_vectorstore()\n",
        "# vector_store.save_index()\n"
      ],
      "metadata": {
        "id": "XVyAdWmX-etQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying And Retrieving Similar Vectors and Their Distances from Query"
      ],
      "metadata": {
        "id": "d6TH0kpn2SdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_vector_store = vector_store\n",
        "loaded_vector_store.load_index()\n",
        "print(\"Num vectores after loading embeddings: \", loaded_vector_store.index.ntotal)"
      ],
      "metadata": {
        "id": "MZZs578B1Sd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkHGO_SkuzBN",
        "outputId": "a2fdc131-0e6b-42f4-dfcf-f444043c8806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Time taken: 0.6179986000061035 seconds *****\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "documents = load_saved_documents(LOAD_DIR)\n",
        "\n",
        "query_text = \"What are the three different ways multi-head attention is utilized in the Transformer model, and how does self-attention in the decoder maintain the auto-regressive property?\"\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "query_embedding = EmbeddingModels(MODEL_NAME, query_text, flag='gpt').embeddings\n",
        "\n",
        "distances, indices = loaded_vector_store.query(query_embedding, top_k=5)\n",
        "\n",
        "chunks = []\n",
        "indices = indices[0]\n",
        "for idx in indices:\n",
        "    chunk_idx = idx\n",
        "    for doc in documents:\n",
        "        chunks.extend(doc.get_chunks())\n",
        "        chunk_idx -= len(doc.get_chunks())\n",
        "\n",
        "retrieved_chunks = [chunks[idx] for idx in indices]\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"*** Time taken: {end_time - start_time} seconds *****\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nIndices: \", indices)\n",
        "print(\"\\nDistances: \", distances[0])\n",
        "for chunk in retrieved_chunks:#\n",
        "    print(\"Chunk: \", chunk)\n",
        "    print(\"     Chars: \", len(chunk.text))\n",
        "    print()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2jF-0eUHxpi",
        "outputId": "908ddcd8-fd4e-44e3-c6fa-6360613352fa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Indices:  [105 109 106 102 573]\n",
            "\n",
            "Distances:  [0.56384414 0.6012912  0.6202766  0.80954206 0.8601507 ]\n",
            "Chunk:  Chunk(Pge-2-Chnk4, Attention Is All You Need, Page 2)\n",
            "     Chars:  1859\n",
            "\n",
            "Chunk:  Chunk(Pge-5-Chnk8, Attention Is All You Need, Page 5)\n",
            "     Chars:  2498\n",
            "\n",
            "Chunk:  Chunk(Pge-3-Chnk5, Attention Is All You Need, Page 3)\n",
            "     Chars:  1826\n",
            "\n",
            "Chunk:  Chunk(Pge-1-Chnk1, Attention Is All You Need, Page 1)\n",
            "     Chars:  2497\n",
            "\n",
            "Chunk:  Chunk(Pge-4-Chnk7, Leave No Context Behind, Page 4)\n",
            "     Chars:  751\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detecting Knowledge in the Retrieved Content"
      ],
      "metadata": {
        "id": "c2a4p0GN72gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rank_bm25 import BM25Okapi\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def detect_knowledge(query_text, retrieved_chunks):\n",
        "\n",
        "    eva_query_embedding = query_embedding\n",
        "    retrieved_chunk_texts = [chunk.text for chunk in retrieved_chunks]\n",
        "    eva_document_embeddings = [chunk.embedding for chunk in retrieved_chunks]\n",
        "\n",
        "    # Computing cosine similarities\n",
        "    cosine_scores = util.cos_sim(eva_query_embedding, eva_document_embeddings)\n",
        "    cosine_normalizer = MinMaxScaler()\n",
        "    cosine_scores_normalized = cosine_normalizer.fit_transform(cosine_scores.cpu().numpy()[0].reshape(-1, 1)).flatten()\n",
        "\n",
        "    # BM25-Based Keyword Matching\n",
        "    tokenized_docs = [doc.split(\" \") for doc in retrieved_chunk_texts]\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "    tokenized_query = query_text.split(\" \")\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # Normalize the Scores\n",
        "    bm25_normalizer = MinMaxScaler()\n",
        "    bm25_scores_normalized = bm25_normalizer.fit_transform(bm25_scores.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Combine Normalized Scores to Create Confidence Scores\n",
        "    confidence_scores = bm25_scores_normalized + cosine_scores_normalized\n",
        "\n",
        "    # Rank all chunks by their confidence scores\n",
        "    ranked_indices = np.argsort(-confidence_scores)\n",
        "\n",
        "    # Discarding Irrelevant Chunks based on confidence score\n",
        "    is_relevant = False\n",
        "    label = \"\"\n",
        "    retained_chunks = []\n",
        "    for idx in ranked_indices:\n",
        "        if (cosine_scores_normalized[idx] != 0) and (bm25_scores_normalized[idx] != 0):\n",
        "            if confidence_scores[idx] >= 1.0:\n",
        "                label = \"Relevant\"\n",
        "                is_relevant = True\n",
        "            elif 0.5 <= confidence_scores[idx] < 1.0:\n",
        "                label = \"Ambiguous\"\n",
        "                is_relevant = True\n",
        "            else:\n",
        "                label = \"Irrelevant\"\n",
        "                is_relevant = False\n",
        "        else:\n",
        "            label = \"Highly Irrelevant\"\n",
        "            is_relevant = False\n",
        "\n",
        "        if is_relevant:\n",
        "            retained_chunks.append(retrieved_chunks[idx])\n",
        "        else:\n",
        "            continue\n",
        "    return retained_chunks\n"
      ],
      "metadata": {
        "id": "KGkY5ns9d_-w"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detecting Knowledge"
      ],
      "metadata": {
        "id": "zKgNSLC-gi0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect Knowledge\n",
        "retained_chunks = detect_knowledge(query_text, retrieved_chunks)\n",
        "\n",
        "# Print Results\n",
        "print(\"Num Chunks before knowledge detection: \", len(retrieved_chunks))\n",
        "print(\"Num Chunks after knowledge detection: \", len(retained_chunks))\n",
        "for retained_subchunk in retained_chunks:\n",
        "    print(\"\\nRetained chunk: \", retained_subchunk)\n",
        "    print(\"     Chars: \", len(retained_subchunk.text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL6wdmOTfP_Q",
        "outputId": "5183da44-c95a-4f2f-f8df-d4f2776eded5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Chunks before knowledge detection:  5\n",
            "Num Chunks after knowledge detection:  3\n",
            "\n",
            "Retained chunk:  Chunk(Pge-5-Chnk8, Attention Is All You Need, Page 5)\n",
            "     Chars:  2498\n",
            "\n",
            "Retained chunk:  Chunk(Pge-3-Chnk5, Attention Is All You Need, Page 3)\n",
            "     Chars:  1826\n",
            "\n",
            "Retained chunk:  Chunk(Pge-2-Chnk4, Attention Is All You Need, Page 2)\n",
            "     Chars:  1859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDWAf9HsV4od"
      },
      "source": [
        "### Evaluating Retrieval Accuracy using Fine-tuned GPT-2 and Trigger for Websearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9OitmOWvYBn9",
        "outputId": "9008e918-9db3-4401-e063-9815f4aa8455"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import softmax\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "def revaluate_retrieved_chunks(query, chunks, model, tokenizer):\n",
        "    # Preparing data for inference\n",
        "    new_test_data = pd.DataFrame({\n",
        "        'Query': [query] * len(chunks),\n",
        "        'Chunk': [chunk.text for chunk in chunks]\n",
        "    })\n",
        "\n",
        "    # Encoding\n",
        "    new_test_inputs = EvaluatorEncoder().encode_data(tokenizer, new_test_data, is_test=True)\n",
        "    new_test_dataset = GPT2Dataset(new_test_inputs)\n",
        "\n",
        "    trainer = Trainer(model=model)\n",
        "\n",
        "    # Predict relevance\n",
        "    new_preds = trainer.predict(new_test_dataset)\n",
        "    logits = new_preds.predictions\n",
        "\n",
        "    # Interpreting Scores\n",
        "    probabilities = softmax(logits, axis=1)\n",
        "    max_probs = probabilities.max(axis=1)\n",
        "\n",
        "    return chunks, max_probs\n",
        "\n",
        "# Function to sort chunks by their accuracy score in descending order\n",
        "def Ranck_chunks(chunks):\n",
        "    return sorted(chunks, key=lambda x: x['accuracy'], reverse=True)\n",
        "\n",
        "# Run\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(arxiv_DIR + '/gpt-model')\n",
        "model = GPT2ForSequenceClassification.from_pretrained(arxiv_DIR + '/gpt-model')\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "query = query_text\n",
        "\n",
        "rated_chunks, accuracy_scores = revaluate_retrieved_chunks(query, retained_chunks, model, tokenizer)\n",
        "\n",
        "# Rerank\n",
        "chunk_dict = []\n",
        "for chunk, score in zip(rated_chunks, accuracy_scores):\n",
        "    chunk_dict.append({\n",
        "        \"chunk\": chunk,\n",
        "        \"accuracy\": f\"{score:.4f}\"\n",
        "    })\n",
        "\n",
        "ranked_retained_chunks = Ranck_chunks(chunk_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query: \", query, \"\\n\")\n",
        "\n",
        "for chunk in ranked_retained_chunks:\n",
        "    print(chunk['chunk'])\n",
        "    print(chunk['accuracy'])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5eY-oi1jnRA",
        "outputId": "c68b97f8-5ae1-43e6-a4f9-859066610ea3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:  What are the three different ways multi-head attention is utilized in the Transformer model, and how does self-attention in the decoder maintain the auto-regressive property? \n",
            "\n",
            "Chunk(Pge-3-Chnk5, Attention Is All You Need, Page 3)\n",
            "0.9955\n",
            "\n",
            "Chunk(Pge-2-Chnk4, Attention Is All You Need, Page 2)\n",
            "0.9907\n",
            "\n",
            "Chunk(Pge-5-Chnk8, Attention Is All You Need, Page 5)\n",
            "0.9893\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge Refinement\n",
        "\n"
      ],
      "metadata": {
        "id": "ticfdZnLkSpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import math\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Decompose the chunk into sentences\n",
        "def decompose_chunk(subchunk):\n",
        "    segments = sent_tokenize(subchunk)\n",
        "    return segments\n",
        "\n",
        "# Evaluate relevance of each segment\n",
        "def evaluate_segments_heuristic(segments, query):\n",
        "    query_terms = query.lower().split()\n",
        "    relevance_scores = []\n",
        "    for segment in segments:\n",
        "        segment_terms = segment.lower().split()\n",
        "        score = sum(1 for term in segment_terms if term in query_terms)\n",
        "        relevance_scores.append(score)\n",
        "    return relevance_scores\n",
        "\n",
        "# Recompose relevant segments\n",
        "def recompose_segments(segments, relevance_scores, threshold=5):\n",
        "    relevant_segments = [segment for segment, score in zip(segments, relevance_scores) if score >= threshold]\n",
        "    recomposed_document = ' '.join(relevant_segments)\n",
        "    return recomposed_document\n",
        "\n",
        "internal_knowledge = []\n",
        "for rankedchunk in ranked_retained_chunks:\n",
        "    if float(rankedchunk['accuracy']) > 0.5:\n",
        "        internal_knowledge.append(rankedchunk['chunk'].text)\n",
        "\n",
        "print(\"internal knowledge: \", len(internal_knowledge))\n",
        "refined_internal_knowledge = []\n",
        "query = query_text\n",
        "for knowledge in internal_knowledge:\n",
        "    segments = decompose_chunk(knowledge)\n",
        "    relevance_scores = evaluate_segments_heuristic(segments, query)\n",
        "    recomposed_document = recompose_segments(segments, relevance_scores)\n",
        "    refined_internal_knowledge.append(recomposed_document)\n",
        "\n",
        "print(\"Len Doc: \", len(refined_internal_knowledge))\n",
        "for i, refined in enumerate(refined_internal_knowledge):\n",
        "    print(\"Before Refine: \", len(ranked_retained_chunks[i]['chunk'].text))\n",
        "    print(\"Text: \", ranked_retained_chunks[i]['chunk'].text[:50])\n",
        "    print(\"Refined chunk Chars: \", len(refined))\n",
        "    print(\"Text: \", refined[:50])\n",
        "    print(\"Accuracy: \", ranked_retained_chunks[i]['accuracy'])\n",
        "    print()\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2yFCouk0kez",
        "outputId": "5297de41-fe7c-4a6f-cf54-ac3facae391f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "internal knowledge:  3\n",
            "Len Doc:  3\n",
            "Before Refine:  1826\n",
            "Text:  Figure 1: The Transformer - model architecture. Th\n",
            "Refined chunk Chars:  956\n",
            "Text:  The Transformer follows this overall architecture \n",
            "Accuracy:  0.9955\n",
            "\n",
            "Before Refine:  1859\n",
            "Text:  and logarithmically for ByteNet. This makes it mor\n",
            "Refined chunk Chars:  939\n",
            "Text:  In the Transformer this is reduced to a constant n\n",
            "Accuracy:  0.9907\n",
            "\n",
            "Before Refine:  2498\n",
            "Text:  output values. These are concatenated and once aga\n",
            "Refined chunk Chars:  1510\n",
            "Text:  These are concatenated and once again projected, r\n",
            "Accuracy:  0.9893\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5DHYEc_RTeF"
      },
      "source": [
        "### Web Searches and scraping for External knowledge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import PyPDF2\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.exceptions import RequestException\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "def google_web_search(query, api_key, cse_id, num_results=3):\n",
        "    scraping_allowed = False\n",
        "    # Function to scrape text from a URL\n",
        "    def __scrape_text_from_url(url):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            scraping_allowed = True\n",
        "        except RequestException as e:\n",
        "            #print(f\"Request denied for {url}: {e}\")\n",
        "            scraping_allowed = False\n",
        "            return \"\"\n",
        "\n",
        "        if scraping_allowed:\n",
        "            content_type = response.headers.get('Content-Type', '').lower()\n",
        "            full_text = \"\"\n",
        "            if 'pdf' in content_type or url.endswith('.pdf'):\n",
        "                # Handle PDF\n",
        "                try:\n",
        "                    with open('temp.pdf', 'wb') as f:\n",
        "                        f.write(response.content)\n",
        "                        pdf_reader = PyPDF2.PdfReader('temp.pdf')\n",
        "                        for page in pdf_reader.pages:\n",
        "                            text = page.extract_text()\n",
        "                            if text:\n",
        "                                full_text += text\n",
        "                    os.remove('temp.pdf')\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to extract text from PDF at {url}: {e}\")\n",
        "                    return \"\"\n",
        "            elif 'html' in content_type:\n",
        "                # Handle HTML\n",
        "                try:\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    paragraphs = soup.find_all('p')\n",
        "                    full_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to extract text from HTML at {url}: {e}\")\n",
        "                    return \"\"\n",
        "            else:\n",
        "                print(f\"Unsupported content type at {url}: {content_type}\")\n",
        "                return \"\"\n",
        "\n",
        "            return full_text\n",
        "\n",
        "    # Google Custom Search API\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=query, cx=cse_id, num=num_results).execute()\n",
        "    search_results = []\n",
        "    for item in res['items']:\n",
        "        result = {\n",
        "            'title': item['title'],\n",
        "            'snippet': item['snippet'],\n",
        "            'link': item['link'],\n",
        "            'text': __scrape_text_from_url(item['link'])\n",
        "            }\n",
        "        search_results.append(result)\n",
        "        print(f\"Title: {result['title']}\")\n",
        "        print(f\"Snippet: {result['snippet']}\")\n",
        "        print(f\"Link: {result['link']}\")\n",
        "        print(f\"Text: {result['text'][:100]}...\")\n",
        "    return search_results\n",
        "\n",
        "# Setup Trigger for Websearch\n",
        "trigger_web = len(retained_chunks) < round((len(retrieved_chunks)/2)+0.01)\n",
        "print(\"Websearch Triggered: \", trigger_web)\n",
        "\n",
        "web_text = []\n",
        "if trigger_web\n",
        "    res = google_web_search(query_text, GOOGLE_SEARCH_API_KEY, GOOGLE_CSE_ID, num_results=3)\n",
        "    for r in res:\n",
        "        if r['text'] != \"\":\n",
        "            web_text.append({\n",
        "                \"title\":r['title'],\n",
        "                \"snippet\":r['snippet'],\n",
        "                \"link\":r['link'],\n",
        "                \"text\":r['text']\n",
        "            })\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    print(\"Num Web Results: \", len(web_text))\n",
        "    print()\n",
        "    for re in web_text:\n",
        "        print(re['title'])\n",
        "        print(re['snippet'])\n",
        "        print(re['link'])\n",
        "        print(re['text'][:100])\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "vtZECavlWwdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4258aa-c48c-43d5-a527-10cadbe8e5c3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Websearch Triggered:  False\n",
            "Title: Understanding Encoder And Decoder LLMs\n",
            "Snippet: Jun 17, 2023 ... Coming back to the original transformer architecture outlined at the beginning of this section, the multi-head self-attention mechanism in the ...\n",
            "Link: https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder\n",
            "Text: Several people asked me to dive a bit deeper into large language model (LLM) jargon and explain some...\n",
            "Title: 7181-attention-is-all-you-need.pdf\n",
            "Snippet: 3 Applications of Attention in our Model. The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the ...\n",
            "Link: https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf\n",
            "Text: Attention Is All You Need\n",
            "Ashish Vaswani\u0003\n",
            "Google Brain\n",
            "avaswani@google.comNoam Shazeer\u0003\n",
            "Google Brain...\n",
            "Title: [Discussion] (Rant) Most of us just pretend to understand ...\n",
            "Snippet: Dec 2, 2021 ... And thus, the birth of multi-headed attention. Although the overall effectiveness is questionable. The Transformer architecture also had other ...\n",
            "Link: https://www.reddit.com/r/MachineLearning/comments/r76igz/discussion_rant_most_of_us_just_pretend_to/\n",
            "Text: ...\n",
            "Num Web Results:  2\n",
            "\n",
            "Understanding Encoder And Decoder LLMs\n",
            "Jun 17, 2023 ... Coming back to the original transformer architecture outlined at the beginning of this section, the multi-head self-attention mechanism in the ...\n",
            "https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder\n",
            "Several people asked me to dive a bit deeper into large language model (LLM) jargon and explain some\n",
            "\n",
            "7181-attention-is-all-you-need.pdf\n",
            "3 Applications of Attention in our Model. The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the ...\n",
            "https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani\u0003\n",
            "Google Brain\n",
            "avaswani@google.comNoam Shazeer\u0003\n",
            "Google Brain\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking Scraped Text from Websearches."
      ],
      "metadata": {
        "id": "ZkuaEyxA2isQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chunk_text(text, chunk_size, overlap_size):\n",
        "    wchunks = []\n",
        "    for i in range(0, len(text), chunk_size - overlap_size):\n",
        "        chunk = text[i:i + chunk_size]\n",
        "        wchunks.append(chunk)\n",
        "\n",
        "    return wchunks\n",
        "\n",
        "chunk_size = 1000\n",
        "overlap_size = 100\n",
        "external_knowledge = []\n",
        "for webtext in web_text:\n",
        "    webchunks = chunk_text(webtext['text'], chunk_size, overlap_size)\n",
        "    external_knowledge.append(webchunks)\n",
        "\n",
        "wchunks = [chunk for sublist in external_knowledge for chunk in sublist]\n",
        "print(\"Num Ext knowledge Records: \", len(external_knowledge))\n",
        "print(\"Num Chunks in 1st Record: \", len(external_knowledge[0]))\n",
        "print(\"Num Chunks in Last Record: \", len(external_knowledge[-1]))\n",
        "print(\"Num of total Chunks: \", len(wchunks))\n",
        "print(\"Num chars in 1st chunk: \", len(wchunks[0]))\n",
        "print(\"Num chars in Last chunk: \", len(wchunks[-1]))\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSXa1Rk10NHD",
        "outputId": "531298b3-2005-40a3-f0ee-0615dc8a498d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Ext knowledge Records:  2\n",
            "Num Chunks in 1st Record:  14\n",
            "Num Chunks in Last Record:  37\n",
            "Num of total Chunks:  51\n",
            "Num chars in 1st chunk:  1000\n",
            "Num chars in Last chunk:  145\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve Similar Web Chunks"
      ],
      "metadata": {
        "id": "PHl5Di5T9shm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "query_text = query_text\n",
        "# embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "query_embedding = np.array(model.encode(query_text)).astype('float32').reshape(1, -1)\n",
        "web_embeddings = np.array(model.encode(wchunks)).astype('float32').reshape(len(wchunks), -1)\n",
        "\n",
        "dimension = web_embeddings.shape[1]\n",
        "web_faiss_index = faiss.IndexFlatL2(dimension)\n",
        "web_faiss_index.add(web_embeddings)\n"
      ],
      "metadata": {
        "id": "vYcgu9iI0s3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Refining Web Data"
      ],
      "metadata": {
        "id": "6JdrjnpklGb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dimension)\n",
        "print(web_faiss_index.ntotal)\n",
        "\n",
        "web_k = 3\n",
        "distances, indices = web_faiss_index.search(query_embedding, web_k)\n",
        "print(\"Distances: \", distances[0])\n",
        "print(\"Indices: \", indices[0])\n",
        "\n",
        "\n",
        "web_retrieved_chunks = []\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    web_retrieved_chunks.append({\n",
        "        \"distance\":f\"{distances[0][i]:.4f}\",\n",
        "        \"text\":wchunks[idx]\n",
        "    })\n",
        "\n",
        "\n",
        "# Refine Web Retrieved Chunks\n",
        "web_knowledge_corpus = web_retrieved_chunks\n",
        "web_refined_knowledge = []\n",
        "query = query_text\n",
        "for knowledge in web_knowledge_corpus:\n",
        "    segments = decompose_chunk(knowledge['text'])\n",
        "    relevance_scores = evaluate_segments_heuristic(segments, query)\n",
        "    recomposed_document = recompose_segments(segments, relevance_scores)\n",
        "    web_refined_knowledge.append(recomposed_document)\n",
        "    print(relevance_scores)\n",
        "\n",
        "print(\"Query: \", query)\n",
        "print(\"Num Retained Chunk Chars: \", len(web_knowledge_corpus[0]['text']))\n",
        "print(\"Num Refined Knowledge Chars: \", len(web_refined_knowledge[0]))\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYIpZOv1OOWd",
        "outputId": "c2dfa39e-c842-42cd-f733-4fbee72040d3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384\n",
            "51\n",
            "Distances:  [0.59839493 0.72529244 0.7294751 ]\n",
            "Indices:  [27  2 19]\n",
            "[2, 17, 5, 3, 1, 10, 5, 11]\n",
            "[5, 5, 5, 7, 9, 3, 5, 4, 0]\n",
            "[3, 5, 0, 8, 5, 2]\n",
            "Query:  What are the three different ways multi-head attention is utilized in the Transformer model, and how does self-attention in the decoder maintain the auto-regressive property?\n",
            "Num Retained Chunk Chars:  1000\n",
            "Num Refined Knowledge Chars:  759\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reconciling the Knowledge Base"
      ],
      "metadata": {
        "id": "EbEABziVlqaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "knowledge_base = []\n",
        "\n",
        "for internal in refined_internal_knowledge:\n",
        "    print(internal)\n",
        "    knowledge_base.append(internal)\n",
        "\n",
        "\n",
        "if trigger_web and len(web_refined_knowledge) > 0:\n",
        "    knowledge_base.append(web_refined_knowledge[0])\n",
        "\n",
        "print(\"Knowledge Base: \", len(knowledge_base))\n",
        "for kn in knowledge_base:\n",
        "    print(len(kn))\n",
        "    print(kn[:100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GteLZCs5H3y",
        "outputId": "68dd0e60-0645-4650-83f1-45937d040ed9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n",
            "In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
            "These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. FFN(x) = max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.\n",
            "Knowledge Base:  3\n",
            "956\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "939\n",
            "In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced\n",
            "1510\n",
            "These are concatenated and once again projected, resulting in the final values, as depicted in Figur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Response Generator"
      ],
      "metadata": {
        "id": "gLZ5-VGQpJPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "llm = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def generate_gpt_response(query, text_chunks):\n",
        "    response = llm.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an AI assistant with expertise in generating meaningful responses from a given context nd base.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Generate a response to this query: {query} based on the following context:\\n{text_chunks}\\n\\nResponse:\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "query = query_text\n",
        "response = generate_gpt_response(query, knowledge_base)\n",
        "print(\"Question: \", query)\n",
        "print()\n",
        "print(\"Response: \", response)\n",
        "print()\n"
      ],
      "metadata": {
        "id": "zsz9S_NgpOYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c5e1a2-a484-4e98-fbd3-69121190f852"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What are the three different ways multi-head attention is utilized in the Transformer model, and how does self-attention in the decoder maintain the auto-regressive property?\n",
            "\n",
            "Response:  In the Transformer model, multi-head attention is utilized in three distinct ways:\n",
            "\n",
            "1. **Encoder-Decoder Attention**: This layer enables the decoder to utilize information from the encoder's output. Here, the queries originate from the previous decoder layer, while the keys and values are derived from the encoder's output. This setup allows each position in the decoder to attend to all positions in the input sequence, effectively integrating contextual information from the entire input.\n",
            "\n",
            "2. **Self-Attention in the Encoder**: In this case, all queries, keys, and values come from the output of the previous layer within the encoder. Each position in the encoder can attend to every other position in the layer, facilitating a comprehensive understanding of the input sequence's characteristics.\n",
            "\n",
            "3. **Self-Attention in the Decoder**: Similar to the encoder's self-attention, this allows each position in the decoder to attend to all preceding positions (up to and including its own position). This is crucial for generating output sequentially while ensuring that the model does not access future information, thus preserving the auto-regressive property during generation.\n",
            "\n",
            "To maintain the auto-regressive property in the decoder, the self-attention mechanism is modified to prevent any position from attending to subsequent positions. By constraining attention in this manner, the model ensures that the generation of each token depends solely on the previously generated tokens, which is key for effective sequential predictions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisation"
      ],
      "metadata": {
        "id": "mx0r0lB0ZUdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "data = {}\n",
        "data['Chunk'] = ['query']\n",
        "for idx in indices:\n",
        "    data['Chunk'].append(f'chunk{idx}')\n",
        "\n",
        "data['Index'] = [0]\n",
        "data['Index'].extend(indices)\n",
        "\n",
        "data['Distance'] = [0.0]\n",
        "data['Distance'].extend(distances[0])\n",
        "\n",
        "data['Chunk_Number'] = []\n",
        "for i in range(len(indices)+1):\n",
        "    data['Chunk_Number'].append(i)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['X'] = df['Index']\n",
        "df['Y'] = df['Distance']\n",
        "df['Z'] = df['Chunk_Number']\n",
        "\n",
        "# Separate the query and chunks\n",
        "df_query = df[df['Chunk'] == 'query']\n",
        "df_chunks = df[df['Chunk'] != 'query']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Adding chunks to the plot (same color for all chunks except query)\n",
        "fig.add_trace(\n",
        "    go.Scatter3d(\n",
        "        x=df_chunks['X'],\n",
        "        y=df_chunks['Y'],\n",
        "        z=df_chunks['Z'],\n",
        "        mode='markers+text',\n",
        "        marker=dict(\n",
        "            size=8,\n",
        "            color='gray',\n",
        "            opacity=0.8,\n",
        "            line=dict(width=1)\n",
        "        ),\n",
        "        text=df_chunks['Chunk'],\n",
        "        hoverinfo='text',\n",
        "        name='Chunks'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter3d(\n",
        "        x=df_query['X'],\n",
        "        y=df_query['Y'],\n",
        "        z=df_query['Z'],\n",
        "        mode='markers+text',\n",
        "        marker=dict(\n",
        "            size=8,              # Same size as the other chunks\n",
        "            color='red',          # Query is red\n",
        "            opacity=1.0,\n",
        "            line=dict(width=2)\n",
        "        ),\n",
        "        text=df_query['Chunk'],\n",
        "        hoverinfo='text',\n",
        "        name='Query'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Update layout for better visualization\n",
        "fig.update_layout(\n",
        "    title='3D Scatter Plot: Query at Origin and Chunks Measured from Start',\n",
        "    scene=dict(\n",
        "        xaxis=dict(\n",
        "            title='Indices',\n",
        "            range=[-50, 700]  # Slight padding around data for better visibility\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title='Distance from Query',\n",
        "            range=[-0.5, 2]\n",
        "        ),\n",
        "        zaxis=dict(\n",
        "            title='Chunk Number',\n",
        "            range=[-1, 6]\n",
        "        ),\n",
        "        aspectratio=dict(x=1, y=1, z=1),  # Keep equal scaling for clarity\n",
        "        camera=dict(\n",
        "            eye=dict(x=1.5, y=1.5, z=1.5)  # Adjust initial view angle for a good overview\n",
        "        )\n",
        "    ),\n",
        "    legend=dict(\n",
        "        title='Legend',\n",
        "        x=0.7,\n",
        "        y=0.9\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "91lb0xc6CpIO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDhF9bgBtiH9whaBV4Timp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
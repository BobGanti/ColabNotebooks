{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BobGanti/ColabNotebooks/blob/main/CRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqyWMYpXfP0Q"
      },
      "source": [
        "# Implementation For CRAG <br>(Corrective Retrieval Augmentation Generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "LGTKY90CqnC6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE1fIXW-d5Cx"
      },
      "source": [
        "#### Installing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsxNHYYxFOoU"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pyarrow==14.0.1 datasets==2.13.0 \\\n",
        "dill \\\n",
        "openai \\\n",
        "requests \\\n",
        "datasets \\\n",
        "accelerate \\\n",
        "beautifulsoup4 \\\n",
        "PyPDF2 PyMuPDF \\\n",
        "faiss-cpu faiss-gpu \\\n",
        "google-api-python-client \\\n",
        "transformers sentence-transformers sentencepiece rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import pickle\n",
        "import pymupdf\n",
        "import uuid\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from torch.utils.data import Dataset\n",
        "import transformers\n",
        "import faiss\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "15ycqqFRMjay"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWvCtQu9c3MI"
      },
      "source": [
        "#### Mounting the drive and setting up the environment variables (Colab specific)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVcr3fX-FZEm",
        "outputId": "0a3e5555-d5fa-42d5-afe0-9a4a23cdcb92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive, userdata\n",
        "drive = drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DATA_DIR = userdata.get('ROOT_DIR')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GOOGLE_SEARCH_API_KEY = userdata.get('GOOGLE_SEARCH_API_KEY')\n",
        "GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
        "arxiv_DIR = DATA_DIR + \"/arxiv\"\n",
        "PDF_DIR = arxiv_DIR + \"/PDFs\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc1Kg4iaq3Ko"
      },
      "source": [
        "### Class Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Lub9Prs0B49L"
      },
      "outputs": [],
      "source": [
        "global_chunk_counter = 0\n",
        "\n",
        "# Chunk class\n",
        "class Chunk:\n",
        "    def __init__(self, text, metadata, embedding):\n",
        "        self.text = text\n",
        "        self.embedding = embedding\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Chunk({self.metadata['chunk_id']}, {self.metadata['title']}, Page {self.metadata['page_number']})\"\n",
        "\n",
        "class Document:\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.title = os.path.splitext(os.path.basename(path))[0]\n",
        "        self.document_id = f\"{self.title.replace(' ', '_')}#{str(uuid.uuid4())[:10]}\"\n",
        "        self.chunks = []\n",
        "        self._extract_and_chunk_text(chunk_size=2500, chunk_overlap=100)\n",
        "\n",
        "\n",
        "    def _extract_and_chunk_text(self, chunk_size=2500, chunk_overlap=100):\n",
        "        global global_chunk_counter\n",
        "        document = pymupdf.open(self.path)\n",
        "        separators = ['\\n\\n', '\\n', ' ', '']\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=separators\n",
        "        )\n",
        "        print(f\"\\n***** Processing: {self.title} *****\\n\")\n",
        "        num_chunks = 0\n",
        "        for num in range(len(document)):\n",
        "            page = document.load_page(num)\n",
        "            page_text = self.get_page_text(page)\n",
        "            page_chunks = text_splitter.split_text(page_text)\n",
        "            for i, chunk in enumerate(page_chunks):\n",
        "                page_number = num + 1\n",
        "                global_chunk_counter += 1\n",
        "\n",
        "                metadata = {\n",
        "                    \"title\": self.title,\n",
        "                    \"document_id\": self.document_id,\n",
        "                    \"page_number\": page_number,\n",
        "                    \"chunk_id\": f\"Pge-{page_number}-Chnk{num_chunks + 1}\",\n",
        "                    \"global_chunk_number\": global_chunk_counter\n",
        "                }\n",
        "                embed = EmbeddingModels(MODEL_NAME, chunk, flag='gpt').embeddings\n",
        "                newchunk = Chunk(chunk, metadata, embed)\n",
        "                self.chunks.append(newchunk)\n",
        "                print(f\"Chunk {global_chunk_counter} created: {metadata}\")\n",
        "                num_chunks += 1\n",
        "        print(f\"Total chunks created: {num_chunks}\")\n",
        "\n",
        "\n",
        "\n",
        "    def remove_section(self, text):\n",
        "        # Heuristic to remove the References section in the pdf file\n",
        "        text = re.split(r'References|Bibliography', text, flags=re.IGNORECASE)[0]\n",
        "        return text\n",
        "\n",
        "    def get_page_text(self, page):\n",
        "        text = page.get_text()\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "        text = self.remove_section(text)\n",
        "        return text\n",
        "\n",
        "    def get_chunks(self):\n",
        "        return self.chunks\n",
        "\n",
        "    def save_document(self, dir):\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "        file_path = os.path.join(dir, f'{self.document_id}.pkl')\n",
        "        with open(file_path, 'wb') as file:\n",
        "            pickle.dump(self, file)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document({self.title}, {len(self.chunks)} chunks)\"\n",
        "\n",
        "# EmbeddingModels class\n",
        "class EmbeddingModels:\n",
        "    def __init__(self, model, text, flag='gpt'):\n",
        "        self.model = model\n",
        "        self.flag = flag\n",
        "        if self.flag == 'gpt':\n",
        "            self.embeddings = self.__create_gpt_embedding(text)\n",
        "        elif self.flag == 'sentence':\n",
        "            self.embeddings = self.__create_sentence_embedding(text)\n",
        "\n",
        "    def __create_gpt_embedding(self, text):\n",
        "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "        embedding = client.embeddings.create(input=[text], model=self.model).data[0].embedding\n",
        "        return embedding\n",
        "\n",
        "    def __create_sentence_embedding(self, text):\n",
        "        embedding = self.model.encode(text, convert_to_tensor=True)\n",
        "        embedding_np = np.array(embedding).astype('float32')\n",
        "        return embedding_np\n",
        "\n",
        "# VectorStore class\n",
        "class VectorStore:\n",
        "    def __init__(self, dimension, index_file=arxiv_DIR + '/Vectors/faiss_index.idx'):\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index_file = index_file\n",
        "        self.metadata_store = []\n",
        "\n",
        "    def add_embeddings(self, embeddings, metadata):\n",
        "        embeddings = np.array(embeddings).astype('float32')\n",
        "        if len(embeddings.shape) == 1:\n",
        "            embeddings = np.expand_dims(embeddings, axis=0)\n",
        "            metadata = [metadata]\n",
        "        self.index.add(embeddings)\n",
        "        self.metadata_store.extend(metadata)\n",
        "\n",
        "    def query(self, embedding, top_k=5):\n",
        "        embedding_np = np.array([embedding]).astype('float32')\n",
        "        vectors = self.load_index()\n",
        "        distances, indices = vectors.search(embedding_np, top_k)\n",
        "        return distances, indices\n",
        "\n",
        "    def save_index(self):\n",
        "        try:\n",
        "            faiss.write_index(self.index, self.index_file)\n",
        "            return \"Index saved to disk.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error saving index: {e}\"\n",
        "\n",
        "\n",
        "    def load_index(self):\n",
        "        self.index = faiss.read_index(self.index_file)\n",
        "        return self.index\n",
        "\n",
        "# Custom dataset class\n",
        "class GPT2Dataset(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
        "        return item\n",
        "\n",
        "class EvaluatorEncoder:\n",
        "    # Function to encode the data using the fine-tuned tokenizer\n",
        "    def encode_data(self, tokenizer, data, max_length=512, is_test=False):\n",
        "        inputs = tokenizer(\n",
        "            [f\"Question: {q} Context: {c}\" for q, c in zip(data['Query'].tolist(), data['Chunk'].tolist())],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        if not is_test:  # Only convert labels if not test data\n",
        "            labels = torch.tensor(data['Relevance'].tolist())\n",
        "            inputs['labels'] = labels\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubs3vZPwSPhD"
      },
      "source": [
        "### PDFs Preprocessing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwlpGES4XFi3"
      },
      "source": [
        "#### Create Documents and Saving them locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "txDmsCLlaB2R"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to read PDFs from a directory and create Document objects\n",
        "def create_documents_from_pdf(pdf_dir, save_dir):\n",
        "    document_paths = [os.path.join(pdf_dir, file) for file in os.listdir(pdf_dir) if file.endswith('.pdf')]\n",
        "    print(\"Number of PDFs: \", len(document_paths))\n",
        "\n",
        "    documents = []\n",
        "    for path in document_paths:\n",
        "        document = Document(path)\n",
        "        document.save_document(save_dir)\n",
        "        documents.append(document)\n",
        "    return documents\n",
        "\n",
        "SAVE_DIR = arxiv_DIR + \"/Documents\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Creating documents and save them locally\n",
        "created_documents = create_documents_from_pdf(PDF_DIR, SAVE_DIR)\n",
        "print(\"Number of Documents Created: \", len(created_documents))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EQuJRFNUKEI-"
      },
      "outputs": [],
      "source": [
        "\n",
        "SAVE_DIR = arxiv_DIR + \"/Documents\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Creating documents and save them locally\n",
        "created_documents = create_documents_from_pdf(PDF_DIR, SAVE_DIR)\n",
        "print(\"Number of Documents Created: \", len(created_documents))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd06uuTDQ9nM"
      },
      "source": [
        "#### Loading Saved Documents from Local Dir and explore its features\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load saved Document objects\n",
        "def load_saved_documents(dir):\n",
        "    documents = []\n",
        "    for filename in os.listdir(dir):\n",
        "        if filename.endswith('.pkl'):\n",
        "            file_path = os.path.join(dir, filename)\n",
        "            with open(file_path, 'rb') as file:\n",
        "                document = pickle.load(file)\n",
        "                documents.append(document)\n",
        "    return documents\n",
        "documents = load_saved_documents(SAVE_DIR)\n",
        "print(\"Number of Documents: \", len(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OqWoKpgFfu_",
        "outputId": "85c2e1e6-f23f-4abe-874a-52afcf8c0014"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents:  23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqxynHGNBuLK"
      },
      "outputs": [],
      "source": [
        "# Directory paths\n",
        "LOAD_DIR = arxiv_DIR + \"/Documents\"\n",
        "MODEL_NAME = \"text-embedding-3-small\"\n",
        "FLAG = 'gpt'\n",
        "\n",
        "# Load documents from the local directory\n",
        "loaded_documents = load_saved_documents(LOAD_DIR)\n",
        "print(\"Number of Documents: \", len(loaded_documents))\n",
        "\n",
        "# Verify embeddings\n",
        "\n",
        "# Printing last record\n",
        "print(loaded_documents[-1].chunks[-1].metadata)\n",
        "print(f\"Embeddings: {loaded_documents[-1].get_chunks()[-1].embedding[:5]}\")\n",
        "print(\"Text: \", loaded_documents[-1].chunks[0].text[:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Store"
      ],
      "metadata": {
        "id": "Z_ET8_rUrGSV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcJxwWE9qTbB"
      },
      "source": [
        "#### Vectorstore Credentials Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectorstore_credentials(FLAG='gpt'):\n",
        "\n",
        "    EmbeddingCredentials = {\n",
        "            'gpt':{\n",
        "                \"MODEL_NAME\":\"text-embedding-3-small\",\n",
        "                \"FAISS_INDEX_FILE\":arxiv_DIR + '/Vectors/faiss_index.idx'\n",
        "            },\n",
        "            \"sentence\":{\n",
        "                \"MODEL_NAME\":'all-MiniLM-L6-v2',\n",
        "                \"FAISS_INDEX_FILE\":arxiv_DIR + '/Vectors/faiss_index_sentence.idx'\n",
        "            }\n",
        "    }\n",
        "    return EmbeddingCredentials\n",
        "\n",
        "FLAG='gpt'\n",
        "EmbeddingCredentials = get_vectorstore_credentials(FLAG)\n",
        "MODEL_NAME = EmbeddingCredentials[FLAG]['MODEL_NAME']\n",
        "FAISS_INDEX_FILE = EmbeddingCredentials[FLAG]['FAISS_INDEX_FILE']\n",
        "os.makedirs(os.path.dirname(FAISS_INDEX_FILE), exist_ok=True)\n",
        "print(MODEL_NAME)\n",
        "print(FAISS_INDEX_FILE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFHhmPHmHCni",
        "outputId": "233b59b2-235c-4ad1-d77e-370c5ce83355"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text-embedding-3-small\n",
            "/content/drive/MyDrive/Datasets/arxiv/Vectors/faiss_index.idx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialising the Vectorstore"
      ],
      "metadata": {
        "id": "LJmX7QNsDqD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising the Vectorstore\n",
        "DIM = len(EmbeddingModels(MODEL_NAME, \"hello\", flag='gpt').embeddings)\n",
        "vector_store = VectorStore(dimension=DIM, index_file=FAISS_INDEX_FILE)\n",
        "\n",
        "vector_store.load_index()\n",
        "vectorstore_records = vector_store.index.ntotal\n",
        "print(\"Num vectores after loading embeddings: \", vectorstore_records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ms-vaZ5RL9f",
        "outputId": "d16c7116-05ad-4b98-e5b7-14337eb33b03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num vectores after loading embeddings:  775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Addiding Documents\n",
        "def add_documents_to_vectorstore():\n",
        "    for doc in loaded_documents:\n",
        "        for chunk in doc.get_chunks():\n",
        "            vector_store.add_embeddings(chunk.embedding, chunk.metadata)\n",
        "\n",
        "    vector_store.save_index()\n",
        "    return vector_store\n",
        "\n",
        "if vectorstore_records == 0:\n",
        "    vector_store = add_documents_to_vectorstore()\n",
        "    vector_store.save_index()\n",
        "\n",
        "print(\"Num vectores after adding documents: \", vector_store.index.ntotal)\n"
      ],
      "metadata": {
        "id": "XVyAdWmX-etQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59946279-4b50-429e-c51b-f15f1cffad22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num vectores after adding documents:  775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying And Retrieving Similar Vectors and Their Distances from Query"
      ],
      "metadata": {
        "id": "d6TH0kpn2SdX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkHGO_SkuzBN",
        "outputId": "03261247-5b8a-4118-8e2f-cccfc23215b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Time taken: 0.5313231945037842 seconds *****\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# query_text = \"What are the three different ways multi-head attention is utilized in the Transformer model, and how does self-attention in the decoder maintain the auto-regressive property?\"\n",
        "query_text = \"What are the three different ways multi-head attention is utilized in the Transformer model, and how does self-attention in the decoder maintain the auto-regressive property?\"\n",
        "documents = loaded_documents\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "query_embedding = EmbeddingModels(MODEL_NAME, query_text, flag='gpt').embeddings\n",
        "\n",
        "distances, indices = vector_store.query(query_embedding, top_k=5)\n",
        "\n",
        "chunks = []\n",
        "indices = indices[0]\n",
        "for idx in indices:\n",
        "    chunk_idx = idx\n",
        "    for doc in documents:\n",
        "        chunks.extend(doc.get_chunks())\n",
        "        chunk_idx -= len(doc.get_chunks())\n",
        "\n",
        "retrieved_chunks = [chunks[idx] for idx in indices]\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"*** Time taken: {end_time - start_time} seconds *****\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nIndices: \", indices)\n",
        "print(\"\\nDistances: \", distances[0])\n",
        "for chunk in retrieved_chunks:#\n",
        "    print(\"Chunk: \", chunk)\n",
        "    print(\"     Chars: \", len(chunk.text))\n",
        "    print()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2jF-0eUHxpi",
        "outputId": "a818718c-dc45-423c-83c9-5f649687c386"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Indices:  [105 109 106 102 573]\n",
            "\n",
            "Distances:  [0.5638442  0.6012912  0.6202766  0.80954206 0.8601509 ]\n",
            "Chunk:  Chunk(Pge-2-Chnk4, Attention Is All You Need, Page 2)\n",
            "     Chars:  1859\n",
            "\n",
            "Chunk:  Chunk(Pge-5-Chnk8, Attention Is All You Need, Page 5)\n",
            "     Chars:  2498\n",
            "\n",
            "Chunk:  Chunk(Pge-3-Chnk5, Attention Is All You Need, Page 3)\n",
            "     Chars:  1826\n",
            "\n",
            "Chunk:  Chunk(Pge-1-Chnk1, Attention Is All You Need, Page 1)\n",
            "     Chars:  2497\n",
            "\n",
            "Chunk:  Chunk(Pge-4-Chnk7, Leave No Context Behind, Page 4)\n",
            "     Chars:  751\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detecting Knowledge in the Retrieved Chunks, discarding Chunks without knowledge"
      ],
      "metadata": {
        "id": "c2a4p0GN72gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rank_bm25 import BM25Okapi\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def detect_knowledge(query_text, retrieved_chunks):\n",
        "\n",
        "    eva_query_embedding = query_embedding\n",
        "    retrieved_chunk_texts = [chunk.text for chunk in retrieved_chunks]\n",
        "    eva_document_embeddings = [chunk.embedding for chunk in retrieved_chunks]\n",
        "\n",
        "    # Computing cosine similarities\n",
        "    cosine_scores = util.cos_sim(eva_query_embedding, eva_document_embeddings)\n",
        "    cosine_normalizer = MinMaxScaler()\n",
        "    cosine_scores_normalized = cosine_normalizer.fit_transform(cosine_scores.cpu().numpy()[0].reshape(-1, 1)).flatten()\n",
        "\n",
        "    # BM25-Based Keyword Matching\n",
        "    tokenized_docs = [doc.split(\" \") for doc in retrieved_chunk_texts]\n",
        "    bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "    tokenized_query = query_text.split(\" \")\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # Normalize the Scores\n",
        "    bm25_normalizer = MinMaxScaler()\n",
        "    bm25_scores_normalized = bm25_normalizer.fit_transform(bm25_scores.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Combine Normalized Scores to Create Confidence Scores\n",
        "    confidence_scores = bm25_scores_normalized + cosine_scores_normalized\n",
        "\n",
        "    # Rank all chunks by their confidence scores\n",
        "    ranked_indices = np.argsort(-confidence_scores)\n",
        "\n",
        "    # Discarding Irrelevant Chunks based on confidence score\n",
        "    is_relevant = False\n",
        "    label = \"\"\n",
        "    retained_chunks = []\n",
        "    for idx in ranked_indices:\n",
        "        if (cosine_scores_normalized[idx] != 0) and (bm25_scores_normalized[idx] != 0):\n",
        "            if confidence_scores[idx] >= 1.0:\n",
        "                label = \"Relevant\"\n",
        "                is_relevant = True\n",
        "            elif 0.5 <= confidence_scores[idx] < 1.0:\n",
        "                label = \"Ambiguous\"\n",
        "                is_relevant = True\n",
        "            else:\n",
        "                label = \"Irrelevant\"\n",
        "                is_relevant = False\n",
        "        else:\n",
        "            label = \"Highly Irrelevant\"\n",
        "            is_relevant = False\n",
        "\n",
        "        if is_relevant:\n",
        "            retained_chunks.append(retrieved_chunks[idx])\n",
        "        else:\n",
        "            continue\n",
        "    return retained_chunks\n"
      ],
      "metadata": {
        "id": "KGkY5ns9d_-w"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect Knowledge\n",
        "retained_chunks = detect_knowledge(query_text, retrieved_chunks)\n",
        "\n",
        "# Print Results\n",
        "print(\"Num Chunks before knowledge detection: \", len(retrieved_chunks))\n",
        "print(\"Num Chunks after knowledge detection: \", len(retained_chunks))\n",
        "for retained_subchunk in retained_chunks:\n",
        "    print(\"\\nRetained chunk: \", retained_subchunk)\n",
        "    print(\"     Chars: \", len(retained_subchunk.text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL6wdmOTfP_Q",
        "outputId": "72492e31-56d8-4dd0-97f2-d72d1c6ee085"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Chunks before knowledge detection:  5\n",
            "Num Chunks after knowledge detection:  3\n",
            "\n",
            "Retained chunk:  Chunk(Pge-5-Chnk8, Attention Is All You Need, Page 5)\n",
            "     Chars:  2498\n",
            "\n",
            "Retained chunk:  Chunk(Pge-3-Chnk5, Attention Is All You Need, Page 3)\n",
            "     Chars:  1826\n",
            "\n",
            "Retained chunk:  Chunk(Pge-2-Chnk4, Attention Is All You Need, Page 2)\n",
            "     Chars:  1859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDWAf9HsV4od"
      },
      "source": [
        "#### Evaluating Retrieval Accuracy using Fine-tuned GPT-2 and Trigger for Websearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9OitmOWvYBn9",
        "outputId": "13a0cde5-6f8b-49ec-c7cd-9ef5aa73b993"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import softmax\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "def revaluate_retrieved_chunks(query, chunks, model, tokenizer):\n",
        "    # Preparing data for inference\n",
        "    new_test_data = pd.DataFrame({\n",
        "        'Query': [query] * len(chunks),\n",
        "        'Chunk': [chunk.text for chunk in chunks]\n",
        "    })\n",
        "\n",
        "    # Encoding\n",
        "    new_test_inputs = EvaluatorEncoder().encode_data(tokenizer, new_test_data, is_test=True)\n",
        "    new_test_dataset = GPT2Dataset(new_test_inputs)\n",
        "\n",
        "    trainer = Trainer(model=model)\n",
        "\n",
        "    # Predict relevance\n",
        "    new_preds = trainer.predict(new_test_dataset)\n",
        "    logits = new_preds.predictions\n",
        "\n",
        "    # Interpreting Scores\n",
        "    probabilities = softmax(logits, axis=1)\n",
        "    max_probs = probabilities.max(axis=1)\n",
        "\n",
        "    chunk_dict = []\n",
        "    for chunk, score in zip(chunks, max_probs):\n",
        "        chunk_dict.append({\n",
        "            \"chunk\": chunk,\n",
        "            \"accuracy\": f\"{score:.4f}\"\n",
        "        })\n",
        "    rated_chunks = sorted(chunk_dict, key=lambda x: x['accuracy'], reverse=True)\n",
        "\n",
        "    return rated_chunks\n",
        "\n",
        "# Run\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(arxiv_DIR + '/gpt-model')\n",
        "model = GPT2ForSequenceClassification.from_pretrained(arxiv_DIR + '/gpt-model')\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "query = query_text\n",
        "\n",
        "rated_chunks = revaluate_retrieved_chunks(query, retained_chunks, model, tokenizer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query: \", query, \"\\n\")\n",
        "\n",
        "for chunk in rated_chunks:\n",
        "    print(chunk['chunk'])\n",
        "    print(chunk['accuracy'])\n",
        "    print('************************')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5eY-oi1jnRA",
        "outputId": "71c608de-279a-448f-bbda-9b8b562be4d2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:  What are the three different ways multi-head attention is utilized in the Transformer model, and how does self-attention in the decoder maintain the auto-regressive property? \n",
            "\n",
            "Chunk(Pge-3-Chnk5, Attention Is All You Need, Page 3)\n",
            "0.9955\n",
            "************************\n",
            "Chunk(Pge-2-Chnk4, Attention Is All You Need, Page 2)\n",
            "0.9907\n",
            "************************\n",
            "Chunk(Pge-5-Chnk8, Attention Is All You Need, Page 5)\n",
            "0.9893\n",
            "************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Knowledge Refinement\n",
        "\n"
      ],
      "metadata": {
        "id": "ticfdZnLkSpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import math\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Decompose the chunk into sentences\n",
        "def decompose_chunk(subchunk):\n",
        "    segments = sent_tokenize(subchunk)\n",
        "    return segments\n",
        "\n",
        "# Evaluate relevance of each segment\n",
        "def evaluate_segments_heuristic(segments, query):\n",
        "    query_terms = query.lower().split()\n",
        "    relevance_scores = []\n",
        "    for segment in segments:\n",
        "        segment_terms = segment.lower().split()\n",
        "        score = sum(1 for term in segment_terms if term in query_terms)\n",
        "        relevance_scores.append(score)\n",
        "    return relevance_scores\n",
        "\n",
        "# Recompose relevant segments\n",
        "def recompose_segments(segments, relevance_scores, threshold=5):\n",
        "    relevant_segments = [segment for segment, score in zip(segments, relevance_scores) if score >= threshold]\n",
        "    recomposed_document = ' '.join(relevant_segments)\n",
        "    return recomposed_document\n",
        "\n",
        "internal_knowledge = []\n",
        "for chunk in rated_chunks:\n",
        "    if float(chunk['accuracy']) > 0.5:\n",
        "        internal_knowledge.append(chunk['chunk'].text)\n",
        "\n",
        "print(\"\\ninternal knowledge: \", len(internal_knowledge))\n",
        "refined_internal_knowledge = []\n",
        "query = query_text\n",
        "for knowledge in internal_knowledge:\n",
        "    segments = decompose_chunk(knowledge)\n",
        "    relevance_scores = evaluate_segments_heuristic(segments, query)\n",
        "    recomposed_document = recompose_segments(segments, relevance_scores)\n",
        "    refined_internal_knowledge.append(recomposed_document)\n",
        "\n",
        "print(\"Len Doc: \", len(refined_internal_knowledge))\n",
        "for i, refined in enumerate(refined_internal_knowledge):\n",
        "    print(\"Before Refine: \", len(rated_chunks[i]['chunk'].text))\n",
        "    print(\"Text: \", rated_chunks[i]['chunk'].text[:50])\n",
        "    print(\"Refined chunk Chars: \", len(refined))\n",
        "    print(\"Text: \", refined[:50])\n",
        "    print(\"Accuracy: \", rated_chunks[i]['accuracy'])\n",
        "    print()\n",
        "    print(\"Text: \\n\", refined)\n",
        "    print()\n",
        "    print(rated_chunks[i]['chunk'].text)\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2yFCouk0kez",
        "outputId": "2c2230ec-5eac-4370-8397-0a6b1f9f34eb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "internal knowledge:  3\n",
            "Len Doc:  3\n",
            "Before Refine:  1826\n",
            "Text:  Figure 1: The Transformer - model architecture. Th\n",
            "Refined chunk Chars:  956\n",
            "Text:  The Transformer follows this overall architecture \n",
            "Accuracy:  0.9955\n",
            "\n",
            "Text: \n",
            " The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.\n",
            "\n",
            "Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
            "Before Refine:  1859\n",
            "Text:  and logarithmically for ByteNet. This makes it mor\n",
            "Refined chunk Chars:  939\n",
            "Text:  In the Transformer this is reduced to a constant n\n",
            "Accuracy:  0.9907\n",
            "\n",
            "Text: \n",
            " In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
            "\n",
            "and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2\n",
            "Before Refine:  2498\n",
            "Text:  output values. These are concatenated and once aga\n",
            "Refined chunk Chars:  1510\n",
            "Text:  These are concatenated and once again projected, r\n",
            "Accuracy:  0.9893\n",
            "\n",
            "Text: \n",
            " These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. FFN(x) = max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer.\n",
            "\n",
            "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW K i , V W V i ) Where the projections are parameter matrices W Q i ∈Rdmodel×dk, W K i ∈Rdmodel×dk, W V i ∈Rdmodel×dv and W O ∈Rhdv×dmodel. In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5DHYEc_RTeF"
      },
      "source": [
        "### Web Searches and scraping for External knowledge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import PyPDF2\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.exceptions import RequestException\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "web_refined_knowledge = []\n",
        "\n",
        "# Setup Trigger for Websearch\n",
        "trigger_web = len(retained_chunks) < round((len(retrieved_chunks)/2)+0.01)\n",
        "print(\"Websearch Triggered: \", trigger_web)\n",
        "\n",
        "def google_web_search(query, api_key, cse_id, num_results=3):\n",
        "    scraping_allowed = False\n",
        "    # Function to scrape text from a URL\n",
        "    def __scrape_text_from_url(url):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            scraping_allowed = True\n",
        "        except RequestException as e:\n",
        "            #print(f\"Request denied for {url}: {e}\")\n",
        "            scraping_allowed = False\n",
        "            return \"\"\n",
        "\n",
        "        if scraping_allowed:\n",
        "            content_type = response.headers.get('Content-Type', '').lower()\n",
        "            full_text = \"\"\n",
        "            if 'pdf' in content_type or url.endswith('.pdf'):\n",
        "                # Handle PDF\n",
        "                try:\n",
        "                    with open('temp.pdf', 'wb') as f:\n",
        "                        f.write(response.content)\n",
        "                        pdf_reader = PyPDF2.PdfReader('temp.pdf')\n",
        "                        for page in pdf_reader.pages:\n",
        "                            text = page.extract_text()\n",
        "                            if text:\n",
        "                                full_text += text\n",
        "                    os.remove('temp.pdf')\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to extract text from PDF at {url}: {e}\")\n",
        "                    return \"\"\n",
        "            elif 'html' in content_type:\n",
        "                # Handle HTML\n",
        "                try:\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    paragraphs = soup.find_all('p')\n",
        "                    full_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to extract text from HTML at {url}: {e}\")\n",
        "                    return \"\"\n",
        "            else:\n",
        "                print(f\"Unsupported content type at {url}: {content_type}\")\n",
        "                return \"\"\n",
        "\n",
        "            return full_text\n",
        "\n",
        "    # Google Custom Search API\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=query, cx=cse_id, num=num_results).execute()\n",
        "    search_results = []\n",
        "    for item in res['items']:\n",
        "        result = {\n",
        "            'title': item['title'],\n",
        "            'snippet': item['snippet'],\n",
        "            'link': item['link'],\n",
        "            'text': __scrape_text_from_url(item['link'])\n",
        "            }\n",
        "        search_results.append(result)\n",
        "        print(f\"Title: {result['title']}\")\n",
        "        print(f\"Snippet: {result['snippet']}\")\n",
        "        print(f\"Link: {result['link']}\")\n",
        "        print(f\"Text: {result['text'][:100]}...\")\n",
        "    return search_results\n",
        "\n",
        "def chunk_text(text, chunk_size, overlap_size):\n",
        "    wchunks = []\n",
        "    for i in range(0, len(text), chunk_size - overlap_size):\n",
        "        chunk = text[i:i + chunk_size]\n",
        "        wchunks.append(chunk)\n",
        "    return wchunks\n",
        "\n",
        "web_text = []\n",
        "if trigger_web:\n",
        "    res = google_web_search(query_text, GOOGLE_SEARCH_API_KEY, GOOGLE_CSE_ID, num_results=3)\n",
        "    for r in res:\n",
        "        if r['text'] != \"\":\n",
        "            web_text.append({\n",
        "                \"title\":r['title'],\n",
        "                \"snippet\":r['snippet'],\n",
        "                \"link\":r['link'],\n",
        "                \"text\":r['text']\n",
        "            })\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    print(\"Num Web Results: \", len(web_text))\n",
        "    print()\n",
        "    for re in web_text:\n",
        "        print(re['title'])\n",
        "        print(re['snippet'])\n",
        "        print(re['link'])\n",
        "        print(re['text'][:100])\n",
        "        print()\n",
        "\n",
        "    # Chunking Scraped Text from Websearches.\n",
        "    chunk_size = 1000\n",
        "    overlap_size = 100\n",
        "    external_knowledge = []\n",
        "    for webtext in web_text:\n",
        "        webchunks = chunk_text(webtext['text'], chunk_size, overlap_size)\n",
        "        external_knowledge.append(webchunks)\n",
        "\n",
        "    wchunks = [chunk for sublist in external_knowledge for chunk in sublist]\n",
        "\n",
        "    # Retrieve Similar Web Chunks\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    query_text = query_text\n",
        "    # embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "    query_embedding = np.array(model.encode(query_text)).astype('float32').reshape(1, -1)\n",
        "    web_embeddings = np.array(model.encode(wchunks)).astype('float32').reshape(len(wchunks), -1)\n",
        "\n",
        "    dimension = web_embeddings.shape[1]\n",
        "    web_faiss_index = faiss.IndexFlatL2(dimension)\n",
        "    web_faiss_index.add(web_embeddings)\n",
        "\n",
        "    # Refining Web Data\n",
        "\n",
        "    web_k = 3\n",
        "    distances, indices = web_faiss_index.search(query_embedding, web_k)\n",
        "    print(\"Distances: \", distances[0])\n",
        "    print(\"Indices: \", indices[0])\n",
        "\n",
        "\n",
        "    web_retrieved_chunks = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        web_retrieved_chunks.append({\n",
        "            \"distance\":f\"{distances[0][i]:.4f}\",\n",
        "            \"text\":wchunks[idx]\n",
        "        })\n",
        "\n",
        "\n",
        "    # Refine Web Retrieved Chunks\n",
        "    web_knowledge_corpus = web_retrieved_chunks\n",
        "    query = query_text\n",
        "    for knowledge in web_knowledge_corpus:\n",
        "        segments = decompose_chunk(knowledge['text'])\n",
        "        relevance_scores = evaluate_segments_heuristic(segments, query)\n",
        "        recomposed_document = recompose_segments(segments, relevance_scores)\n",
        "        web_refined_knowledge.append(recomposed_document)\n",
        "        print(relevance_scores)\n"
      ],
      "metadata": {
        "id": "vtZECavlWwdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98849f3-c9ab-4f24-bbff-2abc065c3498"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Websearch Triggered:  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reconciling the Knowledge Base"
      ],
      "metadata": {
        "id": "ZfhDY4NDrs_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = []\n",
        "\n",
        "if len(refined_internal_knowledge) > 0:\n",
        "    for internal in refined_internal_knowledge:\n",
        "        knowledge_base.append(internal)\n",
        "\n",
        "if len(web_refined_knowledge) > 0:\n",
        "    for web in web_refined_knowledge:\n",
        "        knowledge_base.append(web_refined_knowledge[0])\n",
        "\n",
        "print(\"Knowledge Base: \", len(knowledge_base))\n",
        "for kb in knowledge_base:\n",
        "        print(len(kb))\n",
        "        print(kb[:100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZUJmLLnm_jK",
        "outputId": "6388f2c9-b391-4aff-f271-e44be4c33110"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Base:  3\n",
            "956\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "939\n",
            "In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced\n",
            "1510\n",
            "These are concatenated and once again projected, resulting in the final values, as depicted in Figur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Response Generator"
      ],
      "metadata": {
        "id": "gLZ5-VGQpJPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "llm = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def generate_gpt_response(query, text_chunks):\n",
        "    response = llm.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "         messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an AI assistant with expertise in generating meaningful responses to queries based on given context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Generate a response to the following query, limiting your knowledge on the given context only. context:\\n\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"Query: {query}\\n\\nContext: {text_chunks}\\n\\nResponse:\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "query = query_text\n",
        "context = knowledge_base\n",
        "response = generate_gpt_response(query, knowledge_base)\n",
        "print(\"Question: \", query)\n",
        "print()\n",
        "print(\"Response: \", response)\n",
        "print()\n"
      ],
      "metadata": {
        "id": "zsz9S_NgpOYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb7671e-6356-45fe-cbab-58881600ad8a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  What are the three different ways multi-head attention is utilized in the Transformer model, and how does self-attention in the decoder maintain the auto-regressive property?\n",
            "\n",
            "Response:  In the Transformer model, multi-head attention is utilized in three distinct ways:\n",
            "\n",
            "1. **Encoder-Decoder Attention**: In this layer, the queries come from the decoder's previous layer, while the keys and values are derived from the output of the encoder. This setup enables each position in the decoder to attend to all positions in the input sequence, effectively linking the encoder's output to the decoder's input.\n",
            "\n",
            "2. **Self-Attention in the Encoder**: Here, all the keys, values, and queries originate from the same input, which is the previous layer's output in the encoder. This allows each position in the encoder to attend to all other positions in the encoder, creating a rich internal representation of the input sequence.\n",
            "\n",
            "3. **Self-Attention in the Decoder**: Similar to the encoder's self-attention, this mechanism allows each position in the decoder to attend to all previous positions, including itself. However, it is designed to prevent any leftward information flow, ensuring that the prediction for a given position is based only on the current and past positions. This is crucial for maintaining the auto-regressive property of the model, where each output is generated sequentially based on preceding outputs.\n",
            "\n",
            "Overall, these multi-head attention components work together to enhance the model's ability to capture relationships within the input and output sequences while adhering to the necessary constraints of autoregression in the decoder.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQbRIHdsizO/ZL/g4JPN4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}